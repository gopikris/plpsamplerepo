{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "<table style=\"border: none\" align=\"left\">\n   <tr style=\"border: none\">\n      <th style=\"border: none\"><font face=\"verdana\" size=\"4\" color=\"black\"><b>Use Spark ML and Scala to detect network intrusions</b></font></th>\n      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n   </tr> \n   <tr style=\"border: none\">\n       <td style=\"border: none\"><img src=\"https://github.com/pmservice/wml-sample-models/raw/master/tensorflow/hand-written-digit-recognition/images/experiment_banner.png\" width=\"600\" height = \"200\" alt=\"Icon\"></td>\n   </tr>\n</table>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This notebook shows you how to easily build two classification models using the Spark Machine Learning (ML) library to detect network intrusions. It uses the Random Forest (RF) classifier and the Multilayer Perceptron (MLP) classifier to build the required algorithms.\n\n<a href=\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\" target=\"_blank\" rel=\"noopener noreferrer\">UCI kddcup data</a> (743MB) is used in this notebook. This data set can be audited and provides intrusions simulated in a military network environment. It was originally used for the **The Third International Knowledge Discovery and Data Mining Tools Competition** organized for **KDD-99**. \n\n\nThis notebook runs on Scala 2.11 with Spark 2.3 and it was tested with Watson Studio Spark Environments.\n\n## Table of contents\n\n1. [Download data](#download)<br>\n2. [Load and prepare data](#load)<br>\n3. [Build the models](#build)<br>\n    3.1 [Set up the Random Forest model](#rf)<br>\n    3.2 [Set up the Multilayer Perceptron model](#mlp)<br>\n4.  [Summary and next steps](#summary)  \n\n\n<a id=\"download\"></a>\n## 1. Download data\n\nFirst, download the prerequisite data set from Watson Studio using the following url: <a href=\"https://dataplatform.ibm.com/exchange-api/v1/entries/1438a61212a64ac435c837ba046efc19/data?accessKey=903188bb984a30f38bb889102a7db39f\" target=\"_blank\" rel=\"noopener noreferrer\">https://dataplatform.ibm.com/exchange-api/v1/entries/1438a61212a64ac435c837ba046efc19/data?accessKey=903188bb984a30f38bb889102a7db39f</a> \n\nAssign this URL to the variable `url`.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import sys.process._\n\nval url = \"LINK-TO-DATA-SET-URL\"\nval filename = \"./kddcup.zip\"\ns\"wget $url -O $filename\".!"
        }, 
        {
            "source": "Create a ```kddcup``` directory and **unzip** the file that you downloaded:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\"mkdir ./kddcup\".!\n\"unzip ./kddcup.zip -d ./kddcup/\".!"
        }, 
        {
            "source": "List the content of the unzipped file:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\"ls ./kddcup/\".!"
        }, 
        {
            "source": "To use the entire data set ```kddcup.data``` (743 MB) run **gunzip** to unzip the file to the same directory:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\"gunzip ./kddcup/kddcup.data.gz -d ./kddcup/kddcup.data\".!"
        }, 
        {
            "source": "<a id=\"load\"></a>\n## 2. Load and prepare data\nYou can use the ```SparkSession``` to read the data directly into a dataframe because the data is provided in CSV (comma-separated values) format.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+----+---+---+-----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+\n|_c0|_c1| _c2|_c3|_c4|  _c5|_c6|_c7|_c8|_c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30|_c31|_c32|_c33|_c34|_c35|_c36|_c37|_c38|_c39|_c40|   _c41|\n+---+---+----+---+---+-----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+\n|  0|tcp|http| SF|215|45076|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   1| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   0|   0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n|  0|tcp|http| SF|162| 4528|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   2|   2| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   1|   1| 1.0| 0.0| 1.0| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n|  0|tcp|http| SF|236| 1228|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   1| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   2|   2| 1.0| 0.0| 0.5| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n|  0|tcp|http| SF|233| 2032|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   2|   2| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   3|   3| 1.0| 0.0|0.33| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n|  0|tcp|http| SF|239|  486|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   3|   3| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   4|   4| 1.0| 0.0|0.25| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n+---+---+----+---+---+-----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+\nonly showing top 5 rows\n\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "spark = org.apache.spark.sql.SparkSession@2510b661\ndf = [_c0: int, _c1: string ... 40 more fields]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 7, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[_c0: int, _c1: string ... 40 more fields]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "import org.apache.spark.sql.SparkSession\n\nval spark = SparkSession.\n    builder().\n    getOrCreate()\nval df = spark.\n    read.format(\"csv\").\n    option(\"inferSchema\", \"true\").\n    load(\"./kddcup/kddcup.data\")\ndf.show(5)\n"
        }, 
        {
            "source": "Now take a look at the schema and labels of the last column ```_c41```.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- _c0: integer (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: integer (nullable = true)\n |-- _c5: integer (nullable = true)\n |-- _c6: integer (nullable = true)\n |-- _c7: integer (nullable = true)\n |-- _c8: integer (nullable = true)\n |-- _c9: integer (nullable = true)\n |-- _c10: integer (nullable = true)\n |-- _c11: integer (nullable = true)\n |-- _c12: integer (nullable = true)\n |-- _c13: integer (nullable = true)\n |-- _c14: integer (nullable = true)\n |-- _c15: integer (nullable = true)\n |-- _c16: integer (nullable = true)\n |-- _c17: integer (nullable = true)\n |-- _c18: integer (nullable = true)\n |-- _c19: integer (nullable = true)\n |-- _c20: integer (nullable = true)\n |-- _c21: integer (nullable = true)\n |-- _c22: integer (nullable = true)\n |-- _c23: integer (nullable = true)\n |-- _c24: double (nullable = true)\n |-- _c25: double (nullable = true)\n |-- _c26: double (nullable = true)\n |-- _c27: double (nullable = true)\n |-- _c28: double (nullable = true)\n |-- _c29: double (nullable = true)\n |-- _c30: double (nullable = true)\n |-- _c31: integer (nullable = true)\n |-- _c32: integer (nullable = true)\n |-- _c33: double (nullable = true)\n |-- _c34: double (nullable = true)\n |-- _c35: double (nullable = true)\n |-- _c36: double (nullable = true)\n |-- _c37: double (nullable = true)\n |-- _c38: double (nullable = true)\n |-- _c39: double (nullable = true)\n |-- _c40: double (nullable = true)\n |-- _c41: string (nullable = true)\n\n"
                }
            ], 
            "source": "df.printSchema"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------------+-------+\n|            _c41|  count|\n+----------------+-------+\n|    warezmaster.|     20|\n|          smurf.|2807886|\n|            pod.|    264|\n|           imap.|     12|\n|           nmap.|   2316|\n|   guess_passwd.|     53|\n|        ipsweep.|  12481|\n|      portsweep.|  10413|\n|          satan.|  15892|\n|           land.|     21|\n|     loadmodule.|      9|\n|      ftp_write.|      8|\n|buffer_overflow.|     30|\n|        rootkit.|     10|\n|    warezclient.|   1020|\n|       teardrop.|    979|\n|           perl.|      3|\n|            phf.|      4|\n|       multihop.|      7|\n|        neptune.|1072017|\n+----------------+-------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "df.select(\"_c41\").groupBy(\"_c41\").count().show()"
        }, 
        {
            "source": "According to the <a href=\"http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types\" target=\"_blank\" rel=\"noopener noreferrer\">description</a>, the labels should be recoded into five categories using an SQL query. The new column name ```label_s``` stands for *label in string*.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+-------+\n|label_s|  count|\n+-------+-------+\n|    u2r|     52|\n| normal| 972781|\n|    r2l|   1126|\n|  probe|  41102|\n|    dos|3883370|\n+-------+-------+\n\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "query = \n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "        WHEN 'warez...\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 10, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "SELECT *, \n    CASE _c41 \n        WHEN 'back.' THEN 'dos'\n        WHEN 'buffer_overflow.' THEN 'u2r'\n        WHEN 'ftp_write.' THEN 'r2l'\n        WHEN 'guess_passwd.' THEN 'r2l'\n        WHEN 'imap.' THEN 'r2l'\n        WHEN 'ipsweep.' THEN 'probe'\n        WHEN 'land.' THEN 'dos'\n        WHEN 'loadmodule.' THEN 'u2r'\n        WHEN 'multihop.' THEN 'r2l'\n        WHEN 'neptune.' THEN 'dos'\n        WHEN 'nmap.' THEN 'probe'\n        WHEN 'perl.' THEN 'u2r'\n        WHEN 'phf.' THEN 'r2l'\n        WHEN 'pod.' THEN 'dos'\n        WHEN 'portsweep.' THEN 'probe'\n        WHEN 'rootkit.' THEN 'u2r'\n        WHEN 'satan.' THEN 'probe'\n        WHEN 'smurf.' THEN 'dos'\n        WHEN 'spy.' THEN 'r2l'\n        WHEN 'teardrop.' THEN 'dos'\n        WHEN 'warezclient.' THEN 'r2l'\n        WHEN 'warezmaster.' THEN 'r2l'\n        ELSE 'normal'\nEND AS label_s \nFROM attack"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df.createOrReplaceTempView(\"attack\")\n\nval query = \"\"\"SELECT *, \n    CASE _c41 \n        WHEN 'back.' THEN 'dos'\n        WHEN 'buffer_overflow.' THEN 'u2r'\n        WHEN 'ftp_write.' THEN 'r2l'\n        WHEN 'guess_passwd.' THEN 'r2l'\n        WHEN 'imap.' THEN 'r2l'\n        WHEN 'ipsweep.' THEN 'probe'\n        WHEN 'land.' THEN 'dos'\n        WHEN 'loadmodule.' THEN 'u2r'\n        WHEN 'multihop.' THEN 'r2l'\n        WHEN 'neptune.' THEN 'dos'\n        WHEN 'nmap.' THEN 'probe'\n        WHEN 'perl.' THEN 'u2r'\n        WHEN 'phf.' THEN 'r2l'\n        WHEN 'pod.' THEN 'dos'\n        WHEN 'portsweep.' THEN 'probe'\n        WHEN 'rootkit.' THEN 'u2r'\n        WHEN 'satan.' THEN 'probe'\n        WHEN 'smurf.' THEN 'dos'\n        WHEN 'spy.' THEN 'r2l'\n        WHEN 'teardrop.' THEN 'dos'\n        WHEN 'warezclient.' THEN 'r2l'\n        WHEN 'warezmaster.' THEN 'r2l'\n        ELSE 'normal'\nEND AS label_s \nFROM attack\"\"\"\nval labeled = spark.sql(query)\nlabeled.select(\"label_s\").groupBy(\"label_s\").count().show()"
        }, 
        {
            "source": "Now, build a pipeline to prepare the data before building the models.\n\n**Data preparation pipeline:**\n\n*    StringIndexers: c1, c2, and c3 are categorical strings. They must be indexed first.\n*    OneHotEncoders: When the categorical strings have been indexed, you can use one-hot encoding to the indexed columns.\n*    VectorAssembler: Include the wanted columns and assemble them as a feature vector.\n*    labelIndexer: Another StringIndexer is used to index the label_s column to output it as label column.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "indexer1 = strIdx_a7eb7d249c8b\nindexer2 = strIdx_bae7a99525b3\nindexer3 = strIdx_d2290e2b7896\nencoder1 = oneHot_0ba66b85978c\nencoder2 = oneHot_2cc465e87014\nencoder3 = oneHot_7587f352e24a\nfeaturenames = Array(_c0, v_c1, v_c2, v_c3, _c4, _c5, _c6, _c7, _c8, _c9, _c10, _c11, _c12, _c13, _c14, _c15, _c16, _c17, _c18, _c19, _c22, _c23, _c24, _c25, _c26, _c27, _c28, _c29, _c30, _c31, _c32, _c33...\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "warning: there were three deprecation warnings; re-run with -deprecation for details\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 11, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[_c0, v_c1, v_c2, v_c3, _c4, _c5, _c6, _c7, _c8, _c9, _c10, _c11, _c12, _c13, _c14, _c15, _c16, _c17, _c18, _c19, _c22, _c23, _c24, _c25, _c26, _c27, _c28, _c29, _c30, _c31, _c32, _c33, _c34, _c35, _c36, _c37, _c38, _c39, _c40]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorAssembler, OneHotEncoder}\nimport org.apache.spark.ml.Pipeline\n\nval indexer1 = new StringIndexer()\n  .setInputCol(\"_c1\")\n  .setOutputCol(\"i_c1\")\n//   .setHandleInvalid(\"skip\")\n\nval indexer2 = new StringIndexer()\n  .setInputCol(\"_c2\")\n  .setOutputCol(\"i_c2\")\n//   .setHandleInvalid(\"skip\")\n\nval indexer3 = new StringIndexer()\n  .setInputCol(\"_c3\")\n  .setOutputCol(\"i_c3\")\n//   .setHandleInvalid(\"skip\")\n\nval encoder1 = new OneHotEncoder()\n  .setInputCol(\"i_c1\")\n  .setOutputCol(\"v_c1\")\n\nval encoder2 = new OneHotEncoder()\n  .setInputCol(\"i_c2\")\n  .setOutputCol(\"v_c2\")\n\nval encoder3 = new OneHotEncoder()\n  .setInputCol(\"i_c3\")\n  .setOutputCol(\"v_c3\")\n\nval featurenames = Array(\"_c0\", \"v_c1\", \"v_c2\", \"v_c3\", \"_c4\", \"_c5\", \"_c6\", \n                         \"_c7\", \"_c8\", \"_c9\", \"_c10\", \"_c11\", \"_c12\", \"_c13\", \n                         \"_c14\", \"_c15\", \"_c16\", \"_c17\", \"_c18\", \"_c19\",\n                         \"_c22\", \"_c23\", \"_c24\", \"_c25\", \"_c26\", \"_c27\", \n                         \"_c28\", \"_c29\", \"_c30\", \"_c31\", \"_c32\", \"_c33\", \"_c34\", \n                         \"_c35\", \"_c36\", \"_c37\", \"_c38\", \"_c39\", \"_c40\")\nval assembler = new VectorAssembler()\n  .setInputCols(featurenames)\n  .setOutputCol(\"features\")\n\nval labelIndexer = new StringIndexer()\n  .setInputCol(\"label_s\")\n  .setOutputCol(\"label\")\n\nval pipeline_prepare = new Pipeline()\n  .setStages(Array(indexer1,indexer2,indexer3,encoder1,encoder2,encoder3,assembler,labelIndexer))\n"
        }, 
        {
            "source": "You can now fit and transform the data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "data = [_c0: int, _c1: string ... 49 more fields]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 12, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[_c0: int, _c1: string ... 49 more fields]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val data = pipeline_prepare.fit(labeled).transform(labeled)"
        }, 
        {
            "source": "<a id=\"build\"></a>\n## 3. Build the models\nThis section describes how to build the models.\nBecause of the large amount of data, we can use 60/40 split to mitigate overfitting:\n* 60% for the ```training``` set\n* 40% for the ```testing``` set", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "train = [_c0: int, _c1: string ... 49 more fields]\ntest = [_c0: int, _c1: string ... 49 more fields]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 13, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[_c0: int, _c1: string ... 49 more fields]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val Array(train, test) = data.randomSplit(Array(0.6, 0.4))"
        }, 
        {
            "source": "<a id=\"rf\"></a>\n### 3.1 Set up the Random Forest model\n\nAs the Random Forest (RF) algorithm is provided by Spark ML, you only have to set it up. Train and fit the model to the training data.\n\n**Note:** There are 70 categories in the ```c2``` column. This is larger than the default of ```_MaxBins_```. To avoid errors ```_MaxBins_``` is set to 72, because it has to be larger than the biggest number of categories of all categorical variables. \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Training process takes 124.641922243 secs\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "t = 4439303327041089\nrf = rfc_a37b39f56d8b\nrf_model = RandomForestClassificationModel (uid=rfc_a37b39f56d8b) with 5 trees\nduration = 124.641922243\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 14, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "124.641922243"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\nval t = System.nanoTime\nval rf = new RandomForestClassifier()\n  .setLabelCol(\"label\")\n  .setFeaturesCol(\"features\")\n  .setNumTrees(5)\n  .setMaxBins(72)\n\nval rf_model = rf.fit(train)\nval duration = (System.nanoTime - t) / 1e9d\nprintln(s\"Training process takes $duration secs\")"
        }, 
        {
            "source": "Details about the random forest classification model can be printed and will look something like the following:\n```\nRandomForestClassificationModel (uid=rfc_a37b39f56d8b) with 5 trees\n  Tree 0 (weight 1.0):\n    If (feature 1 in {0.0})\n     If (feature 104 <= 0.325)\n      If (feature 116 <= 0.005)\n       If (feature 100 <= 0.375)\n        If (feature 113 <= 0.025)\n         Predict: 1.0\n        Else (feature 113 > 0.025)\n         Predict: 2.0\n       Else (feature 100 > 0.375)\n        If (feature 110 <= 0.14500000000000002)\n         Predict: 0.0\n        Else (feature 110 > 0.14500000000000002)\n         Predict: 0.0\n      Else (feature 116 > 0.005)\n       If (feature 0 <= 0.5)\n        If (feature 105 <= 0.41500000000000004)\n         Predict: 0.0\n        Else (feature 105 > 0.41500000000000004)\n         Predict: 2.0\n       Else (feature 0 > 0.5)\n        If (feature 110 <= 0.025)\n         Predict: 1.0\n        Else (feature 110 > 0.025)\n         Predict: 2.0\n     Else (feature 104 > 0.325)\n      If (feature 73 in {0.0})\n       If (feature 90 <= 0.5)\n        If (feature 83 <= 17.0)\n         Predict: 1.0\n        Else (feature 83 > 17.0)\n         Predict: 1.0\n       Else (feature 90 > 0.5)\n        If (feature 87 <= 1.5)\n         Predict: 1.0\n        Else (feature 87 > 1.5)\n         Predict: 0.0\n      Else (feature 73 not in {0.0})\n       If (feature 108 <= 143.5)\n        If (feature 111 <= 0.095)\n         Predict: 0.0\n        Else (feature 111 > 0.095)\n         Predict: 0.0\n       Else (feature 108 > 143.5)\n        If (feature 110 <= 0.045)\n         Predict: 1.0\n        Else (feature 110 > 0.045)\n         Predict: 0.0\n    Else (feature 1 not in {0.0})\n     If (feature 100 <= 0.01)\n      If (feature 111 <= 0.245)\n       If (feature 110 <= 0.015)\n        Predict: 1.0\n       Else (feature 110 > 0.015)\n        If (feature 108 <= 18.5)\n         Predict: 1.0\n        Else (feature 108 > 18.5)\n         Predict: 0.0\n      Else (feature 111 > 0.245)\n       If (feature 99 <= 48.5)\n        If (feature 115 <= 0.005)\n         Predict: 2.0\n        Else (feature 115 > 0.005)\n         Predict: 0.0\n       Else (feature 99 > 48.5)\n        If (feature 108 <= 24.5)\n         Predict: 2.0\n        Else (feature 108 > 24.5)\n         Predict: 0.0\n     Else (feature 100 > 0.01)\n      Predict: 1.0\n      ...\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "rf_model.toDebugString"
        }, 
        {
            "source": "Then, check the error and accuracy. Notice that the model is very good!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Test Error of RF = 0.006327153168961042\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "evaluator = mcEval_4e127ff5dadb\naccuracy = 0.993672846831039\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 16, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.993672846831039"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val rf_predictions = rf_model.transform(test)\n\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nval evaluator = new MulticlassClassificationEvaluator()\n  .setLabelCol(\"label\")\n  .setPredictionCol(\"prediction\")\n  .setMetricName(\"accuracy\")\nval accuracy = evaluator.evaluate(rf_predictions)\nprintln(\"Test Error of RF = \" + (1.0 - accuracy))"
        }, 
        {
            "source": "<a id=\"mlp\"></a>\n### 3.2 Set up the Multilayer Perceptron model\nBefore building the Multilayer Perceptron (MLP) model, you need to know how many nodes are required for the input layer. \n\nCheck the length of feature vector:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+--------------------+\n|            features|\n+--------------------+\n|(117,[1,10,72,82,...|\n|(117,[1,10,72,82,...|\n|(117,[1,10,72,82,...|\n|(117,[1,10,72,82,...|\n|(117,[1,10,72,82,...|\n+--------------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "train.select(\"features\").show(5)"
        }, 
        {
            "source": "The **input** layer should have 117 nodes and the **output** layer should have ```5``` (5 label categories). This definition also contains an additional hidden layer with ```10``` nodes to build the model. You can change the definition of hidden layer(s).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "layers = Array(117, 8, 5)\nmlp = mlpc_ce78101a560d\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 18, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "mlpc_ce78101a560d"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nval layers = Array[Int](117, 10, 5)\nval mlp = new MultilayerPerceptronClassifier()\n  .setLayers(layers)\n  .setBlockSize(128)\n  .setSeed(1234L)\n  .setMaxIter(25)"
        }, 
        {
            "source": "Now train and fit the model to the training data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Training process takes 89.397066077 secs\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "t = 4439491555134199\nmlp_model = mlpc_ce78101a560d\nduration = 89.397066077\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 19, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "89.397066077"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val t = System.nanoTime\nval mlp_model = mlp.fit(train)\nval duration = (System.nanoTime - t) / 1e9d\nprintln(s\"Training process takes $duration secs\")"
        }, 
        {
            "source": "Now check the performance of the MLP model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Test Error of MLP = 0.015756382401562186\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "mlp_predictions = [_c0: int, _c1: string ... 52 more fields]\naccuracy = 0.9842436175984378\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 20, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.9842436175984378"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val mlp_predictions = mlp_model.transform(test)\nval accuracy = evaluator.evaluate(mlp_predictions)\nprintln(\"Test Error of MLP = \" + (1.0 - accuracy))"
        }, 
        {
            "source": "<a id=\"summary\"></a>\n## 4. Summary and next steps     \nThis notebook shows how to build two well-performing models using the Spark environment in Watson Studio. It is easy to build models using the Spark API and Watson Studio. Just provision the Spark environment, create the notebook, and you are ready to write your code!\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Citations\n\nDua, D. and Karra Taniskidou, E. (2017). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Author\n\n**Bufan Zeng** is a Data Scientist in IBM and a member of the Watson Studio offering management team.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Copyright \u00a9 IBM Corp. 2018. This notebook and its source code are released under the terms of the MIT License.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Scala 2.11 with Spark", 
            "name": "scala", 
            "language": "scala"
        }, 
        "language_info": {
            "mimetype": "text/x-scala", 
            "version": "2.11.8", 
            "name": "scala", 
            "pygments_lexer": "scala", 
            "file_extension": ".scala", 
            "codemirror_mode": "text/x-scala"
        }
    }, 
    "nbformat": 4
}